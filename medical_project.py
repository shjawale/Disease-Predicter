# -*- coding: utf-8 -*-
"""Medical_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zobqBzKpKboNTKGg1iadqZlR-OdaxTpn
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
from transformers import AutoModel, AutoTokenizer
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
# %matplotlib inline

tokenizer = AutoTokenizer.from_pretrained("medicalai/ClinicalBERT")
model = AutoModel.from_pretrained("medicalai/ClinicalBERT")

"""#get data"""

df = pd.read_json('augmented_notes_30K.jsonl', lines = True)

total_size = df.shape[0]
df['note'][0] == df['full_note'][0]
print(df.iloc[:3,:])
print()

train_set = df['full_note'][10]
print(train_set)
print(len(train_set))

classifier = torch.nn.Linear(768, 2)

mymodel = torch.nn.Sequential(model, classifier)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(mymodel.parameters(), lr=2e-5)

train_set_size = 1000
#train_set = df['full_note'][train_set_size]
train_set = df.iloc[:train_set_size,:]
#print(train_set.head())

test_set_size = 100
#test_set = df['full_note'][test_set_size]
test_set = df.iloc[train_set_size:train_set_size+test_set_size,:]

train_loader = DataLoader(train_set, batch_size=16, shuffle=True)
test_loader = DataLoader(test_set, batch_size=16)

print(train_loader.dataset.head())
print()

def train(model, optimizer, train_loader, criterion):
    model.train()
    total_loss = 0

    for batch in train_loader:
        print('i')
        optimizer.zero_grad()
        print(batch)
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f'Training loss: {total_loss/len(train_loader)}')

def evaluate(model, test_loader, criterion):
    model.eval()
    total_loss = 0
    total_acc = 0

    with torch.no_grad():
        for batch in test_loader:
            input_ids, attention_mask, labels = batch
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            total_acc += (predictions == labels).sum().item()

    print(f'Test loss: {total_loss/len(test_loader)} Test acc: {total_acc/len(test_set)*100}%')

for epoch in range(1):
    train(mymodel, optimizer, train_loader, criterion)
    evaluate(mymodel, test_loader, criterion)

"""###Medicines dataset"""

dfM = pd.read_csv('medicines.csv')

num_labels = 1
classifier = torch.nn.Linear(768, num_labels)

mymodelM = torch.nn.Sequential(model, classifier)

criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(mymodel.parameters(), lr=2e-5)

train_set_sizeM = 1000
train_setM = dfM.iloc[:train_set_sizeM,:]
print(train_setM.head())

test_set_size = 100
test_set = df.iloc[train_set_sizeM:train_set_sizeM+test_set_size,:]

train_loader = DataLoader(train_set, batch_size=16, shuffle=True)
test_loader = DataLoader(test_set, batch_size=16)

def train(model, optimizer, train_loader, criterion):
    model.train()
    total_loss = 0

    for batch in train_loader:
        print('i')
        optimizer.zero_grad()
        print(batch)
        input_ids, attention_mask, labels = batch
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f'Training loss: {total_loss/len(train_loader)}')

def evaluate(model, test_loader, criterion):
    model.eval()
    total_loss = 0
    total_acc = 0

    with torch.no_grad():
        for batch in test_loader:
            input_ids, attention_mask, labels = batch
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            predictions = torch.argmax(outputs, dim=1)
            total_acc += (predictions == labels).sum().item()

    print(f'Test loss: {total_loss/len(test_loader)} Test acc: {total_acc/len(test_set)*100}%')

print(train_loader.dataset.head())
print()

for epoch in range(1):
    train(mymodel, optimizer, train_loader, criterion)
    evaluate(mymodel, test_loader, criterion)

"""##sentence embeddings"""

sentence = "I have headaches, dizzyness, and nausea. i have a history of anemia."

def embedding_sentences(model, tokenizer, sentence):
    inputs = tokenizer(sentence, return_tensors="pt")  # Turn text into one-hot vectors

    # Forward pass, get hidden states
    with torch.no_grad():
        outputs = model(**inputs)
    return outputs.last_hidden_state[0].mean(dim=0)

sentence_embedding = [embedding_sentences(model, tokenizer, sentence)]
sentence_embedding_torch = torch.stack(sentence_embedding, 0)
print(sentence_embedding)
print()

training_set_size = 1000
training_set = df['full_note'][training_set_size]
print(training_set[1])

training_set = [embedding_sentences(model, tokenizer, s) for s in training_set[1]]
training_set = torch.stack(training_set, 0)
training_set.shape





from transformers import AutoTokenizer, AutoModelForCausalLM
tokenizer = AutoTokenizer.from_pretrained("starmpcc/Asclepius-7B")
model = AutoModelForCausalLM.from_pretrained("starmpcc/Asclepius-7B")

prompt = """You are an intelligent clinical languge model.
Below is a snippet of patient's discharge summary and a following instruction from healthcare professional.
Write a response that appropriately completes the instruction.
The response should provide the accurate answer to the instruction, while being concise.

[Discharge Summary Begin]
{note}
[Discharge Summary End]

[Instruction Begin]
{question}
[Instruction End]
"""

note = training_set[0]
question = "What is the diagnosis?"

model_input = prompt.format(note=note, question=question)
input_ids = tokenizer(model_input, return_tensors="pt").input_ids
output = model.generate(input_ids)
print(tokenizer.decode(output[0]))

"""###Sources for ClinicalBERT

1.   Wang, G., Liu, X., Ying, Z. et al. Optimized glycemic control of type 2 diabetes with reinforcement learning: a proof-of-concept trial. Nat Med (2023). https://doi.org/10.1038/s41591-023-02552-9

2.   Wang, G., Liu, X., Liu, H., Yang, G. et al. A Generalist Medical Language Model for Disease Diagnosis Assistance. Nat Med (2025). https://doi.org/10.1038/s41591-024-03416-6


###Sources for Asclepius: Synthetic Clincal Notes & Instruction Dataset
@misc{kweon2023publicly,
    title={Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes},
    author={Sunjun Kweon and Junu Kim and Jiyoun Kim and Sujeong Im and Eunbyeol Cho and Seongsu Bae and Jungwoo Oh and Gyubok Lee and Jong Hak Moon and Seng Chan You and Seungjin Baek and Chang Hoon Han and Yoon Bin Jung and Yohan Jo and Edward Choi},
    year={2023},
    eprint={2309.00237},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}


"""

import difflib
for i,s in enumerate(difflib.ndiff(df['note'][0], df['full_note'][0])):
    if s[0]==' ':
        continue
    elif s[0]=='-':
        print(u'Delete "{}" from position {}'.format(s[-1],i))
    elif s[0]=='+':
        print(u'Add "{}" to position {}'.format(s[-1],i))
print()

import difflib
for i,s in enumerate(difflib.ndiff(df['note'][0], df['full_note'][0])):
    if s[0]==' ':
        continue
    #elif s[0]=='-':
    #    print(u'Delete "{}" from position {}'.format(s[-1],i), end=' ')
    elif s[0]=='+':
        print(u'{}'.format(s[-1]), end='')
print()